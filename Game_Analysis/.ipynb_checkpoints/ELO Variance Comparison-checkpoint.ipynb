{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of relationship between ELO and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in packages, graph, and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "import re\n",
    "import chess\n",
    "import subprocess\n",
    "import timeit\n",
    "import pickle\n",
    "import stockfish\n",
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import chess.engine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "engine = chess.engine.SimpleEngine.popen_uci(\"../stockfish-10-64\")\n",
    "stockfish = Stockfish(\"../stockfish-10-64\")\n",
    "#engine = chess.engine.SimpleEngine.popen_uci(path+\"/\"+\"stockfish-11-nnue/src/stockfish\")\n",
    "# engine.configure({\"EvalFile\": path+\"/\"+\"stockfish-11-nnue/src/nn-82215d0fd0df.nnue\"})\n",
    "# engine.configure({\"Analysis Contempt\": \"Off\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_gpickle('../graph-pickle/690k_30.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varWeighted(scores, counts):\n",
    "    if (len(counts)<2):\n",
    "        return None\n",
    "    else:\n",
    "        weightedMean = sum([a*b for a,b in zip(scores,counts)])/sum(counts)\n",
    "        scoreDiffs = [(score - weightedMean)**2 for score in scores]\n",
    "        V1 = sum(counts)\n",
    "        V2 = sum([count**2 for count in counts])\n",
    "        var = (V1/(V1**2-V2)) * sum([a*b for a,b in zip(counts,scoreDiffs)])\n",
    "        return var\n",
    "    \n",
    "def get_score(san):\n",
    "    board = chess.Board()\n",
    "    for move in san:\n",
    "        try:\n",
    "            board.push_san(move)\n",
    "        except:\n",
    "            break\n",
    "    currFen = board.fen()\n",
    "    score = engine.analyse(board, chess.engine.Limit(time=.05), info=chess.engine.INFO_SCORE)\n",
    "    if('#' in str(score['score'])):\n",
    "        if('-' in str(score['score'])):\n",
    "            return -39765\n",
    "        else:\n",
    "            return 39765\n",
    "    else:\n",
    "        score = int(str(score[\"score\"]))\n",
    "    return score\n",
    "\n",
    "def san_to_fen(san):\n",
    "    board = chess.Board()\n",
    "    for move in san:\n",
    "        try:\n",
    "            board.push_san(move)\n",
    "        except:\n",
    "            break\n",
    "    currFen = board.fen()\n",
    "    return currFen\n",
    "\n",
    "def get_node_sd(node):\n",
    "    scores = []\n",
    "    counts = []\n",
    "    try:\n",
    "        for neighbor in list(g.neighbors(node)):\n",
    "            scores.append(nx.get_node_attributes(g, 'score')[neighbor])\n",
    "            counts.append(nx.get_node_attributes(g, 'movelistCount')[neighbor][node])\n",
    "        return node, np.sqrt(varWeighted(scores,counts))\n",
    "    except:\n",
    "        return node, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get score and Variance \n",
    "for each move in san, then split by black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sd_score_from_san(san):\n",
    "    board = chess.Board()\n",
    "    sd_list = []\n",
    "    score_list = []\n",
    "    for move in san:\n",
    "        try:\n",
    "            board.push_san(move)\n",
    "            currFen = board.fen()\n",
    "            board = chess.Board(currFen)\n",
    "            sf = engine.analyse(board, chess.engine.Limit(time=0.05))\n",
    "            \n",
    "            if('#' in str(sf['score'])):\n",
    "                if('-' in str(sf['score'])):\n",
    "                    sf =  -39765\n",
    "                else:\n",
    "                    sf =  39765\n",
    "            else:\n",
    "                sf = int(str(sf[\"score\"]))*-1\n",
    "            score_list.append(sf)\n",
    "            scores = []\n",
    "            counts = []\n",
    "            for neighbor in list(g.neighbors(currFen)):\n",
    "                scores.append(nx.get_node_attributes(g, 'score')[neighbor])\n",
    "                counts.append(nx.get_node_attributes(g, 'count')[neighbor])\n",
    "\n",
    "            sd_list.append(np.sqrt(varWeighted(scores, counts)))\n",
    "        except:\n",
    "            break\n",
    "    return pd.Series([sd_list[::2],sd_list[1::2], score_list[::2],score_list[1::2]]) \n",
    "        ##white, black sd, white, black score\n",
    "#     return(sd_list, score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in pickled df\n",
    "This df has a sample of 2000 games taken from a flattened pgn file (in 'Build DF from PGN\" script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(\"./DF_sd_sf_2000.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate max SD and ELO difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['max_white_sd'] = df_test.sd_white.apply(lambda x: 0 if len(x) == 0 else max(x))\n",
    "df_test['max_black_sd'] = df_test.sd_black.apply(lambda x: 0 if len(x) == 0 else max(x))\n",
    "df_test['mean_white_sd'] = df_test.sd_white.apply(lambda x: 0 if len(x) == 0 else np.mean(x))\n",
    "df_test['mean_black_sd'] = df_test.sd_black.apply(lambda x: 0 if len(x) == 0 else np.mean(x))\n",
    "df_test['max_white_score'] = df_test.score_white.apply(lambda x: 0 if len(x) == 0 else max(x))\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['WhiteElo'] = df_test['WhiteElo'].apply(pd.to_numeric)\n",
    "df_test['BlackElo'] = df_test['BlackElo'].apply(pd.to_numeric, errors= 'coerce')\n",
    "df_test['elo_diff'] = df_test['WhiteElo'] - df_test['BlackElo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Better players play riskier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"CORR:\",df_test['elo_diff'].corr(df_test['max_white_sd']))\n",
    "df_test['log_sd'] = df_test.max_white_sd.apply(lambda x: np.log(x))\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_test['elo_diff'],df_test['log_sd'])\n",
    "print(\"slope:\", slope,\"95% CI: (\", slope - 2*std_err, slope+2*std_err,\")\")\n",
    "ax = sns.regplot('elo_diff' , 'log_sd', data = df_test, line_kws={'color': 'red'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: Plot relationship between ELO diff and max SF score (move quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df_test = df_test[df_test['elo_diff'].notna()]\n",
    "print(\"CORR:\",df_test['elo_diff'].corr(df_test['max_white_score']))\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_test['elo_diff'],df_test['max_white_score'])\n",
    "print(\"slope:\", slope,\"95% CI: (\", slope - 2*std_err, slope+2*std_err,\")\")\n",
    "ax = sns.regplot('elo_diff' , 'max_white_score', data = df_test, line_kws={'color': 'red'})\n",
    "#plt.ylim(0,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot relationships between ELO difference and mean/max SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"CORR:\",df_test['elo_diff'].corr(df_test['max_white_sd']))\n",
    "ax = sns.regplot('elo_diff' , 'max_white_sd', data = df_test)\n",
    "#plt.ylim(0,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = df_test[df_test['max_white_sd'] <500]\n",
    "df_test2['log_sd'] = df_test2.max_white_sd.apply(lambda x: np.log(x))\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_test2['WhiteElo'],df_test2['log_sd'])\n",
    "print(\"slope:\", slope,\"95% CI: (\", slope - 2*std_err, slope+2*std_err,\")\")\n",
    "print(\"CORR:\",df_test2['WhiteElo'].corr(df_test2['log_sd']))\n",
    "ax = sns.regplot('WhiteElo' , 'log_sd', data = df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "print(\"CORR:\",df_test['elo_diff'].corr(df_test['mean_white_sd']))\n",
    "ax = sns.regplot('elo_diff' , 'mean_white_sd', data = df_test)\n",
    "#plt.ylim(0,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = df_test[df_test['mean_white_sd'] <500]\n",
    "df_test2 = df_test2[df_test2['elo_diff'].notna()]\n",
    "df_test2['log_sd'] = df_test2.mean_white_sd.apply(lambda x: np.log(x))\n",
    "print(\"CORR:\",df_test2['elo_diff'].corr(df_test2['log_sd']))\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_test2['elo_diff'],df_test2['log_sd'])\n",
    "print(\"slope:\", slope,\"95% CI: (\", slope - 2*std_err, slope+2*std_err,\")\")\n",
    "ax = sns.regplot('elo_diff' , 'log_sd', data = df_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does stockfish eval correlate with riskiness?\n",
    "Make a list of each move's riskiness, and get corresponding stockfish eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove any lists without risk evals\n",
    "df_short = df_test[df_test['sd_white'].apply(lambda x: len(x)>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim sf lists until they match length of sd\n",
    "def match_length(l1,l2):\n",
    "    if(len(l2)>len(l1)):\n",
    "        while len(l2)>len(l1):\n",
    "            l2.pop()\n",
    "    return(l2)\n",
    "    \n",
    "df_short['white_short'] = df_short.apply(lambda x: match_length(x['sd_white'], x['score_white']), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for s in df_short['white_short']:\n",
    "    for i in s:\n",
    "        score.append(i)\n",
    "        \n",
    "print(len(score))\n",
    "\n",
    "sd = []\n",
    "for s in df_short['sd_white']:\n",
    "    for i in s:\n",
    "        sd.append(i)\n",
    "        \n",
    "print(len(sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(score,sd)\n",
    "print(\"slope:\", slope, \"95% CI: (\", slope - 2*std_err, slope+2*std_err,\")\")\n",
    "print(pearsonr(score,sd)[0])\n",
    "plt.scatter(score,np.log(sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does opponent's previous move quality affect riskiness?\n",
    "Get list of black riskiness as long as white sd\n",
    "\n",
    "then plot black riskiness off white score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short['white_prev'] = df_short.apply(lambda x: match_length(x['sd_black'], x['score_white']), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for s in df_short['white_prev']:\n",
    "    for i in s:\n",
    "        score.append(i)\n",
    "        \n",
    "print(len(score))\n",
    "\n",
    "sd = []\n",
    "for s in df_short['sd_black']:\n",
    "    for i in s:\n",
    "        if i == 0:\n",
    "            i = 1\n",
    "        sd.append(i)\n",
    "        \n",
    "print(len(sd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pearsonr(score,sd)[0])\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(score,np.log(sd))\n",
    "print(\"slope\", slope,\"95% CI: (\", slope - 2*std_err, slope+2*std_err,\")\")\n",
    "d = pd.DataFrame(list(zip(score, np.log(sd))), \n",
    "               columns =['score_of_opp_move', 'log_sd']) \n",
    "ax = sns.regplot('score_of_opp_move' , 'log_sd', data = d, line_kws={'color': 'red'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
